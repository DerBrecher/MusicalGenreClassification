{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All foldernames\n",
    "ORIGINAL_FOLDERS = [\"Dark_Forest/\", \"Full_On/\", \"Goa/\", \"Hi_Tech/\"] # The folders with the original songs\n",
    "CLIP_FOLDERS = [\"Dark_Forest_Clips/\", \"Full_On_Clips/\", \"Goa_Clips/\", \"Hi_Tech_Clips/\"] # The folders with the songs divided into clips\n",
    "FEATURE_FOLDERS = [\"Dark_Forest_Features/\", \"Full_On_Features/\", \"Goa_Features/\", \"Hi_Tech_Features/\"] # The folders with the extracted features from the clips\n",
    "\n",
    "# Variables for dividing the songs into clips\n",
    "CLIP_TIME = 30                        # 30 second clips\n",
    "SAMPLE_RATE = 44100                   # 44100 samples/sec\n",
    "CLIP_SIZE = CLIP_TIME * SAMPLE_RATE   # = 1313000 samples/clip\n",
    "\n",
    "# Number of total songs\n",
    "n_dark_forest_songs = len(os.listdir(ORIGINAL_FOLDERS[0]))\n",
    "n_full_on_songs = len(os.listdir(ORIGINAL_FOLDERS[1]))\n",
    "n_goa_songs = len(os.listdir(ORIGINAL_FOLDERS[2]))\n",
    "n_hi_tech_songs = len(os.listdir(ORIGINAL_FOLDERS[3]))\n",
    "\n",
    "N_SONGS = n_dark_forest_songs + n_full_on_songs + n_goa_songs + n_hi_tech_songs\n",
    "\n",
    "# Number of clips/features\n",
    "n_dark_forest_clips = len(os.listdir(CLIP_FOLDERS[0]))\n",
    "n_full_on_clips = len(os.listdir(CLIP_FOLDERS[1]))\n",
    "n_goa_clips = len(os.listdir(CLIP_FOLDERS[2]))\n",
    "n_hi_tech_clips = len(os.listdir(CLIP_FOLDERS[3]))\n",
    "\n",
    "N_CLIPS = n_dark_forest_clips + n_full_on_clips + n_goa_clips + n_hi_tech_clips\n",
    "N_FEATURES = N_CLIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature matrices using an example file, each feature is normalized individually\n",
    "folder = FEATURE_FOLDERS[0]\n",
    "filenames = os.listdir(folder)\n",
    "    \n",
    "# \"Unjsonify\" the file to a dictionary\n",
    "obj_text = codecs.open(folder + filenames[0], 'r', encoding='utf-8').read()\n",
    "dictionary = json.loads(obj_text)\n",
    "\n",
    "# Initialize all matrices\n",
    "# Librosa Tempo\n",
    "matrix_librosatempo = np.empty(N_FEATURES)\n",
    "\n",
    "# MFCC\n",
    "rows, cols = np.array(dictionary[\"mfcc\"]).shape\n",
    "matrix_mfcc = np.empty((N_FEATURES, rows, cols))\n",
    "\n",
    "# Melspectrogram\n",
    "rows, cols = np.array(dictionary[\"melspectrogram\"]).shape\n",
    "matrix_melspec = np.empty((N_FEATURES, rows, cols))\n",
    "\n",
    "# Chroma STFT\n",
    "rows, cols = np.array(dictionary[\"chroma_stft\"]).shape\n",
    "matrix_chroma = np.empty((N_FEATURES, rows, cols))\n",
    "\n",
    "# Spectral Centroid\n",
    "rows, cols = np.array(dictionary[\"spectral_centroid\"]).shape\n",
    "matrix_centroid = np.empty((N_FEATURES, rows, cols))\n",
    "\n",
    "# add kalman\n",
    "matrix_kalmantempo = np.empty(N_FEATURES)\n",
    "\n",
    "# Genre\n",
    "matrix_genres = np.empty(N_FEATURES)\n",
    "\n",
    "print(matrix_librosatempo.shape)\n",
    "print(matrix_kalmantempo.shape)\n",
    "print(matrix_mfcc.shape)\n",
    "print(matrix_melspec.shape)\n",
    "print(matrix_chroma.shape)\n",
    "print(matrix_centroid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0         # Keeps track of the pointer\n",
    "\n",
    "# Iteration over folders and files\n",
    "for folder in FEATURE_FOLDERS:\n",
    "    filenames = os.listdir(folder)\n",
    "    for file in filenames:\n",
    "        # \"Unjsonify\" the file to a dictionary\n",
    "        obj_text = codecs.open(folder + file, 'r', encoding='utf-8').read()\n",
    "        dictionary = json.loads(obj_text)\n",
    "        \n",
    "        # Insert features into matrices\n",
    "        matrix_librosatempo[count] = dictionary[\"librosa_tempo\"]\n",
    "        matrix_mfcc[count] = dictionary[\"mfcc\"]\n",
    "        matrix_melspec[count] = dictionary[\"melspectrogram\"]\n",
    "        matrix_chroma[count] = dictionary[\"chroma_stft\"]\n",
    "        matrix_centroid[count] = dictionary[\"spectral_centroid\"]\n",
    "        \n",
    "        # add kalman\n",
    "        matrix_kalmantempo[count] = dictionary[\"kalman_tempo\"]\n",
    "        \n",
    "        matrix_genres[count] = dictionary[\"genre\"]\n",
    "        \n",
    "        # Increase counter\n",
    "        count += 1\n",
    "        if (count % (25) == 0):\n",
    "            print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 40\n",
    "\n",
    "matrix_librosatempo_df = matrix_librosatempo[:157]\n",
    "matrix_librosatempo_fo = matrix_librosatempo[157:157+163]\n",
    "matrix_librosatempo_go = matrix_librosatempo[157+163:157+163+175]\n",
    "matrix_librosatempo_ht = matrix_librosatempo[157+163+175:]\n",
    "\n",
    "plt.hist(matrix_librosatempo_df, bins=n_bins);\n",
    "plt.hist(matrix_librosatempo_fo, bins=n_bins);\n",
    "plt.hist(matrix_librosatempo_go, bins=n_bins);\n",
    "plt.hist(matrix_librosatempo_ht, bins=n_bins);\n",
    "plt.legend([\"Dark Forest\", \"Full On\", \"Goa\", \"Hi Tech\"]);\n",
    "plt.title(\"Distribution of librosa BPM\");\n",
    "plt.xlabel(\"BPM\");\n",
    "plt.ylabel(\"Amount of Clips\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    #Check Array size\n",
    "    assert a.shape[0] == b.shape[0], \"First dimensions of matrix must be the same\"\n",
    "    \n",
    "    #save state of \"random\"\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    #set state of random in order to get the same distrubtion as before\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    return (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding of genre\n",
    "Y_org = to_categorical(matrix_genres)\n",
    "\n",
    "# take a window in order to have fewer data per classification but more samples to train the network on\n",
    "window_size = 200\n",
    "offset = 0\n",
    "\n",
    "#take the first slice out of melspec_matrix\n",
    "X = matrix_melspec[:,:,offset:window_size]\n",
    "Y = Y_org\n",
    "\n",
    "while offset + 2*window_size < matrix_melspec.shape[2]:\n",
    "    #shift the window\n",
    "    offset += window_size\n",
    "    \n",
    "    #take a new slice\n",
    "    X2 = matrix_melspec[:,:,offset:offset + window_size]\n",
    "    \n",
    "    #stack the features and the coresponding genres\n",
    "    X = np.vstack((X, X2))\n",
    "    Y = np.vstack((Y, Y_org))\n",
    "\n",
    "#calculate the mean and standard deviation\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "#match array shapes\n",
    "mean_3d = np.repeat(mean[np.newaxis,:,:], X.shape[0], axis=0)\n",
    "std_3d = np.repeat(mean[np.newaxis,:,:], X.shape[0], axis=0)\n",
    " \n",
    "#substract mean and divide\n",
    "X = np.divide(np.subtract(X, mean_3d), std_3d)\n",
    "\n",
    "X = X.reshape(-1, 128, window_size, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show how many samples we have per genre\n",
    "genres_after_windowing = np.argmax(Y, axis=1)\n",
    "print(\"After windowing/spliting\")\n",
    "print(\"Samples of Dark Forest \" + str(np.count_nonzero(genres_after_windowing == 0)))\n",
    "print(\"Samples of Full On \" + str(np.count_nonzero(genres_after_windowing == 1)))\n",
    "print(\"Samples of Goa \" + str(np.count_nonzero(genres_after_windowing == 2)))\n",
    "print(\"Samples of Hi Tech \" + str(np.count_nonzero(genres_after_windowing == 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create shuffled data\n",
    "X_shuffle, Y_shuffle = shuffle_in_unison(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = int(Y.shape[0] * 0.6) # split between test data and validation data\n",
    "n_split2 = int(Y.shape[0] * 0.8) # split bewteen validation data and final test data\n",
    "\n",
    "#split features and real genre into train, test and final validation data set\n",
    "X_train = X_shuffle[:n_split]\n",
    "X_test = X_shuffle[n_split:n_split2]\n",
    "X_final = X_shuffle[n_split2:]\n",
    "Y_train = Y_shuffle[:n_split]\n",
    "Y_test = Y_shuffle[n_split:n_split2]\n",
    "Y_final = Y_shuffle[n_split2:]\n",
    "\n",
    "#print the size of each data set\n",
    "print(str(X_train.shape[0]) + \" samples to train on\")\n",
    "print(str(X_test.shape[0]) + \" samples to test on\")\n",
    "print(str(X_final.shape[0]) + \" samples for final validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(128,window_size,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the final data, which the network has never seen before\n",
    "n_final = X_final.shape[0]\n",
    "\n",
    "#predict genre\n",
    "prediction = model.predict(X_final)\n",
    "prediction_genre = np.argmax(prediction, axis=1)\n",
    "\n",
    "#real genre\n",
    "real_genre = np.argmax(Y_final, axis=1)\n",
    "\n",
    "# accuracy \n",
    "final_acc = (n_final - np.count_nonzero(real_genre - prediction_genre))/n_final\n",
    "print(\"The Overall final Accuracy is: \" + str(final_acc*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate confusion matrix\n",
    "cm = confusion_matrix(real_genre, prediction_genre, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "#display confusion matrix\n",
    "labels = [\"Dark Forest\", \"Full On\", \"Goa\", \"Hi Tech\"]\n",
    "fig = plt.figure(figsize=(8, 6));\n",
    "ax = fig.add_subplot(111);\n",
    "cax = ax.matshow(cm);\n",
    "\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
    "\n",
    "plt.title('Confusion matrix of the classifier');\n",
    "ax.set_xticklabels([\" \"] + labels);\n",
    "ax.set_yticklabels([\" \"] + labels);\n",
    "plt.xlabel('Predicted');\n",
    "plt.ylabel('True');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the sample size of each genre of the final validation data\n",
    "print(\"Samples of genre in final validation Data\")\n",
    "print(\"Samples of Dark Forest \" + str(np.count_nonzero(real_genre == 0)))\n",
    "print(\"Samples of Full On \" + str(np.count_nonzero(real_genre == 1)))\n",
    "print(\"Samples of Goa \" + str(np.count_nonzero(real_genre == 2)))\n",
    "print(\"Samples of Hi Tech \" + str(np.count_nonzero(real_genre == 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN/melspec_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"CNN/melspec_200\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
