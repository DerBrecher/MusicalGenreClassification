{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#was not able to install essentia on windows but got it running under ubuntu\n",
    "import essentia.standard as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_file(file):\n",
    "    features, features_frames = es.MusicExtractor(lowlevelStats=['mean', 'stdev'],\n",
    "                                                  rhythmStats=['mean', 'stdev'],\n",
    "                                                  tonalStats=['mean', 'stdev'])(file)\n",
    "    \n",
    "    #exclude unnecessary features and features with length vary bpm\n",
    "    exclude = [\"rhythm.beats_position\",\n",
    "               \"rhythm.beats_count\",\n",
    "               \"metadata.audio_properties.analysis.downmix\",\n",
    "               \"metadata.audio_properties.codec\",\n",
    "               \"metadata.audio_properties.md5_encoded\",\n",
    "               \"metadata.tags.file_name\",\n",
    "               \"metadata.version.essentia\",\n",
    "               \"metadata.version.essentia_git_sha\",\n",
    "               \"etadata.version.extractor\",\n",
    "               \"tonal.chords_key\",\n",
    "               \"tonal.chords_scale\",\n",
    "               \"tonal.key_edma.key\",\n",
    "               \"tonal.key_edma.scale\",\n",
    "               \"tonal.key_krumhansl.key\",\n",
    "               \"tonal.key_krumhansl.scale\",\n",
    "               \"onal.key_temperley.key\",\n",
    "               \"tonal.key_temperley.scale\",\n",
    "               \"metadata.version.extractor\",\n",
    "               \"tonal.key_temperley.key\"]\n",
    "    \n",
    "    features_names = [feature for feature in features.descriptorNames() if feature not in exclude]\n",
    "    \n",
    "    #flatten the feature list to get a feature vector\n",
    "    feature_list = []\n",
    "    for feature_name in features_names:\n",
    "        feature = features[feature_name]\n",
    "        if (type(feature) is np.ndarray):\n",
    "            feature_list += feature.flatten().tolist()\n",
    "        else:\n",
    "            feature_list.append(feature)\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all features\n",
    "def create_feature_namelist(file):\n",
    "    features, features_frames = es.MusicExtractor(lowlevelStats=['mean', 'stdev'],\n",
    "                                                  rhythmStats=['mean', 'stdev'],\n",
    "                                                  tonalStats=['mean', 'stdev'])(file)\n",
    "    \n",
    "    exclude = [\"rhythm.beats_position\",\n",
    "               \"rhythm.beats_count\",\n",
    "               \"metadata.audio_properties.analysis.downmix\",\n",
    "               \"metadata.audio_properties.codec\",\n",
    "               \"metadata.audio_properties.md5_encoded\",\n",
    "               \"metadata.tags.file_name\",\n",
    "               \"metadata.version.essentia\",\n",
    "               \"metadata.version.essentia_git_sha\",\n",
    "               \"etadata.version.extractor\",\n",
    "               \"tonal.chords_key\",\n",
    "               \"tonal.chords_scale\",\n",
    "               \"tonal.key_edma.key\",\n",
    "               \"tonal.key_edma.scale\",\n",
    "               \"tonal.key_krumhansl.key\",\n",
    "               \"tonal.key_krumhansl.scale\",\n",
    "               \"onal.key_temperley.key\",\n",
    "               \"tonal.key_temperley.scale\",\n",
    "               \"metadata.version.extractor\",\n",
    "               \"tonal.key_temperley.key\"]\n",
    "    \n",
    "    features_names = [feature for feature in features.descriptorNames() if feature not in exclude]\n",
    "    \n",
    "    name_list = []\n",
    "    i = 0 \n",
    "    feature_list = []\n",
    "    for feature_name in features_names:\n",
    "        feature = features[feature_name]\n",
    "        if (type(feature) is np.ndarray):\n",
    "            feature_list += feature.flatten().tolist()\n",
    "        else:\n",
    "            feature_list.append(feature)\n",
    "        while len(name_list) < len(feature_list):\n",
    "            name_list.append((i, feature))\n",
    "            i += 1\n",
    "    \n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if feature extraction works\n",
    "audio_file = \"data/Dark_Forest_clips/03 - Dohm & Schizoid Bears - Modulation Manipulation- Clip2.wav\"\n",
    "\n",
    "start = time.time()\n",
    "features = extract_features_from_file(audio_file)\n",
    "stop = time.time()\n",
    "print(stop - start)\n",
    "len(features)\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the names of the features\n",
    "audio_file = \"data/Dark_Forest_clips/03 - Dohm & Schizoid Bears - Modulation Manipulation- Clip2.wav\"\n",
    "feature_name_list = create_feature_namelist(audio_file)\n",
    "feature_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features for all clips\n",
    "CLIP_FOLDERS = [\"data/Dark_Forest_clips/\", \"data/Full_On_clips/\", \"data/Goa_clips/\", \"data/Hi_Tech_clips/\"]\n",
    "\n",
    "feature_data = []\n",
    "y = []\n",
    "i = 0\n",
    "\n",
    "for folder in CLIP_FOLDERS:\n",
    "    filenames = os.listdir(folder)\n",
    "#     filenames = filenames[:10]\n",
    "    for file in filenames:\n",
    "        print(i)\n",
    "        print(file)\n",
    "#         y.append(CLIP_FOLDERS.index(folder))\n",
    "#         feature = extract_features_from_file(folder + file)\n",
    "#         feature_data.append(feature)\n",
    "#         print(len(feature))\n",
    "        i += 1\n",
    "#         if (i%10 == 0):\n",
    "#             print(i)\n",
    "\n",
    "#create Data Frame to work on\n",
    "X_to_save = pd.DataFrame(feature_data)\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "#duplicate Data Frame to work on\n",
    "X_org = X_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to csv\n",
    "X_to_save.to_csv(\"essentia_features.csv\", index=False)\n",
    "y.to_csv(\"genre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from csv\n",
    "X_org = pd.read_csv(\"essentia_features.csv\")\n",
    "y_org = pd.read_csv(\"genre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero mena and unit variance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_org)\n",
    "y = np.array(y_org)\n",
    "\n",
    "### delete all clips of the first two songs of each genre\n",
    "validation_set_no = np.arange(0, 30) \n",
    "validation_set_no = np.concatenate((validation_set_no, np.arange(157,187)))\n",
    "validation_set_no = np.concatenate((validation_set_no, np.arange(320,355)))\n",
    "validation_set_no = np.concatenate((validation_set_no, np.arange(495,521)))\n",
    "\n",
    "X_final_validation = np.take(X, validation_set_no, 0)\n",
    "X = np.delete(X, validation_set_no, 0)\n",
    "\n",
    "y_final_validation = np.take(y, validation_set_no, 0)\n",
    "y = np.delete(y, validation_set_no, 0)\n",
    "\n",
    "### reduce feature vector\n",
    "#X = X[:,:200]\n",
    "\n",
    "### delete the best ten features\n",
    "# X = np.delete(X, (867, 96, 875, 119, 347, 871, 475, 173, 613, 75), 1)\n",
    "# X_final_validation = np.delete(X_final_validation, (867, 96, 875, 119, 347, 871, 475, 173, 613, 75), 1)\n",
    "\n",
    "### only take the best ten features\n",
    "#X = np.take(X, (867, 96, 875, 119, 347, 871, 475, 173, 613, 75), 1)\n",
    "# X_final_validation = np.take(X_final_validation, (867, 96, 875, 119, 347, 871, 475, 173, 613, 75), 1)\n",
    "\n",
    "### plot shape of X\n",
    "print(X.shape)\n",
    "\n",
    "### split Output and Input data into train and test data (data also gets shuffeld)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tran XGBoost\n",
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 2,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 4} \n",
    "\n",
    "steps = 20  # The number of training iterations\n",
    "\n",
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuarcy\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display confusion matrix\n",
    "labels = [\"Dark Forest\", \"Full On\", \"Goa\", \"Hi Tech\"]\n",
    "fig = plt.figure(figsize=(8, 6));\n",
    "ax = fig.add_subplot(111);\n",
    "cax = ax.matshow(cm);\n",
    "\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
    "\n",
    "plt.title('Confusion matrix of the classifier');\n",
    "ax.set_xticklabels([\" \"] + labels);\n",
    "ax.set_yticklabels([\" \"] + labels);\n",
    "plt.xlabel('Predicted');\n",
    "plt.ylabel('True');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final validation use with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "D_validation = xgb.DMatrix(X_final_validation)\n",
    "\n",
    "preds = model.predict(D_validation)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_final_validation, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_final_validation, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_final_validation, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display confusion matrix\n",
    "labels = [\"Dark Forest\", \"Full On\", \"Goa\", \"Hi Tech\"]\n",
    "fig = plt.figure(figsize=(8, 6));\n",
    "ax = fig.add_subplot(111);\n",
    "cax = ax.matshow(cm);\n",
    "\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
    "\n",
    "plt.title('Confusion matrix of the classifier');\n",
    "ax.set_xticklabels([\" \"] + labels);\n",
    "ax.set_yticklabels([\" \"] + labels);\n",
    "plt.xlabel('Predicted');\n",
    "plt.ylabel('True');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the most important features\n",
    "plot_importance(model, max_num_features=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_list = pd.read_csv(\"feature_names.csv\")\n",
    "feature_name_list.loc[867]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
